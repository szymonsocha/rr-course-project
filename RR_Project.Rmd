---
title: "Reproducible Research Project"
author: "Robert Kowalczyk, Szymon Socha, Karolina Szczęsna"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
    toc: true
    theme: cayman
    highlight: github
---

<style>
body {
text-align: justify
}
details {
  overflow: hidden;
  transition: max-height 0.5s ease-out;
  max-height: 30px;
}
details[open] {
  max-height: 500px;
}
details summary {
  cursor: pointer;
  font-weight: normal;
  font-style: italic;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

This is an RMarkdown presentation that serves as a final project for the course Reproducible Research.\

The project's topic involves transcribing a ready-made [Python notebook](https://www.kaggle.com/code/anshigupta01/flight-price-prediction/notebook?fbclid=IwAR2vtpE8xwc7nhPqDwdEiwwImw62_XuIxTgEkG0Qo2OCCyHIONRIv7DD3pM), downloaded from the Kaggle platform, into R.\

Additionally, the analysis will be expanded to include an additional machine learning model.\
In order to make this project reproducible, we utilize the `dotenv` package to store the environment state.

Please note that under each cell, there is a button available to expand the rewritten Python code.

Team members:

- Robert Kowalczyk
- Szymon Socha
- Karolina Szczęsna


## Comments on reproducibility

Overall, the main goal of this project, therefore reproducing the aforementioned notebook from Python to R, was reached. However, it is worth mentioning that some challenges were encountered, while reproducing the project to a different programming language. Firstly, the original code lacked specific comments and detailed descriptions of the steps in the notebook (plot descriptions, argumentation to choose certain models etc.). Our project has been expanded to include this. The original code had mistakes and was not optimal, specifically in Preprocessing part. While transcribing the notebook to R, certain code snippets were cleaned and optimized to be more readable and efficient. Another challenge that occurred was the difference in results for selected models. In most cases the measurement metrics did not varied significantly, however the distribution of residuals proved more prediction errors than expected.

One of the potential cause of subtle differences in results is the fact that Python and R are different programming languages with their own syntax, libraries, and conventions. Translating code from one language to another can introduce some variances, especially when it comes to functions, data structures, and library-specific implementations. These differences could affect the overall behavior of the code. Other than that, different  libraries in Python and R have different default hyperparameters for machine learning models, leading to variations in the trained models and their predictions.


```{r}
suppressPackageStartupMessages(library(tidyverse))
library(readxl)
library(dplyr)
library(ggplot2)
library(DT)
library(ggpubr)
#library(caret)
#library(gridExtra)
```
<details>
  <summary>Python code</summary>
  ```python
  #importing libraries
  import pandas as pd 
  import numpy as np 
  import seaborn as sns 
  import matplotlib.pyplot as plt
  from sklearn.preprocessing import StandardScaler
  from sklearn.model_selection import train_test_split,GridSearchCV
  from sklearn.metrics import accuracy_score,confusion_matrix
  
  import warnings
  warnings.filterwarnings('ignore')
  ```
</details>

## Importing dataset

```{r}
df <- read_excel('data/Data_Train.xlsx')
head(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df=pd.read_excel('/kaggle/input/flight-fare-prediction-mh/Data_Train.xlsx')
  df.head()
  ```
</details>

```{r}
str(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.info()
  ```
</details>

```{r}
summary(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.describe()
  ```
</details>

```{r}
dim(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.shape
  ```
</details>

```{r}
df %>% 
  summarise_all(~sum(is.na(.)))
```
<details>
  <summary>Python code</summary>
  ```python
  df.isnull().sum()
  ```
</details>

```{r}
df %>% 
  summarise_all(~ sum(is.na(.))) %>% 
  pivot_longer(everything(), names_to = "colname", values_to = "n_of_nas") %>%
  ggplot(., aes(x = colname, y=n_of_nas)) +
  geom_bar(stat = 'identity', fill = 'steelblue') +
  labs(x = 'Column', y = 'No. of NAs')
```
<details>
  <summary>Python code</summary>
  ```python
  import missingno as msno
  msno.bar(df)
  plt.show
  ```
</details>

```{r}
df <- df %>% drop_na()
```
<details>
  <summary>Python code</summary>
  ```python
  df.dropna(inplace=True)
  ```
</details>

```{r}
df %>% summarise_all(~sum(is.na(.)))
```
<details>
  <summary>Python code</summary>
  ```python
  df.isnull().sum()
  ```
</details>

### Data Cleaning

```{r}
glimpse(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.dtypes 
  ```
</details>

```{r}
names(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.columns
  ```
</details>

```{r}
df$Date_of_Journey <- as.POSIXct(df$Date_of_Journey, format="%d/%m/%Y")
df$Dep_Time <- as.POSIXct(df$Dep_Time, format="%H:%M")
df$Arrival_Time <- as.POSIXct(df$Arrival_Time, format="%H:%M")
```
<details>
  <summary>Python code</summary>
  ```python
  def change_into_datetime(col):
      df[col]=pd.to_datetime(df[col])
      
  for i in ['Date_of_Journey','Dep_Time', 'Arrival_Time']:
    change_into_datetime(i)
  ```
</details>

```{r}
glimpse(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.dtypes
  ```
</details>

```{r}
df$journey_day <- as.numeric(format(df$Date_of_Journey, "%d")) # extract day from Date_of_Journey `column`
df$journey_month <- as.numeric(format(df$Date_of_Journey, "%m")) # extract month from Date_of_Journey `column`
```
<details>
  <summary>Python code</summary>
  ```python
  df['journey_day']=df['Date_of_Journey'].dt.day
  df['journey_month']=df['Date_of_Journey'].dt.month
  ```
</details>

```{r}
head(df, 10)
```
<details>
  <summary>Python code</summary>
  ```python
  df.head(10)
  ```
</details>

```{r}
df <- df %>% select(-Date_of_Journey) # remove Date_of_Journey column
```
<details>
  <summary>Python code</summary>
  ```python
  df.drop('Date_of_Journey', axis=1, inplace=True)
  ```
</details>

```{r}
# extract hour and add a new column with the extracted hour values to the data frame
extract_hour <- function(data, col){
  # extract hour using the hour() function from the lubridate package and add a new column with the extracted values to the data frame
  data[[paste0(col, '_hour')]] <- hour(data[[col]])
  # return the modified data frame
  return(data)
}

# extract minute and add a new column with the extracted minute values to the data frame
extract_min <- function(data, col){
  # extract minute using the minute() function from the lubridate package and add a new column with the extracted values to the data frame
  data[[paste0(col, '_min')]] <- minute(data[[col]])
  # return the modified data frame
  return(data)
}

# funtion to drop a column from the data frame
drop_col <- function(data, col){
  data[[col]] <- NULL
  return(data)
}
```
<details>
  <summary>Python code</summary>
  ```python
  def extract_hour(data,col):
    data[col+'_hour']=data[col].dt.hour
    
  def extract_min(data,col):
      data[col+'_min']=data[col].dt.minute
      
  def drop_col(data,col):
      data.drop(col,axis=1,inplace=True)
  ```
</details>

```{r}
df <- extract_hour(df, 'Dep_Time')
df <- extract_min(df, 'Dep_Time')
df <- drop_col(df, 'Dep_Time')
```
<details>
  <summary>Python code</summary>
  ```python
  # Departure time is when a plane leaves the gate. 
  # Similar to Date_of_Journey we can extract values from Dep_Time
  extract_hour(df,'Dep_Time')
  
  #extracting minutes
  extract_min(df,'Dep_Time')
  
  #drop the column
  drop_col(df,'Dep_Time')
  ```
</details>

```{r}
df <- extract_hour(df, 'Arrival_Time')
df <- extract_min(df, 'Arrival_Time')
df <- drop_col(df, 'Arrival_Time')
```
<details>
  <summary>Python code</summary>
  ```python
  #extracting hour
  extract_hour(df,'Arrival_Time')
  
  #extracting min
  extract_min(df,'Arrival_Time')
  
  #drop the column
  drop_col(df,'Arrival_Time')
  ```
</details>

```{r}
head(df, 10)
```
<details>
  <summary>Python code</summary>
  ```python
  df.head(10)
  ```
</details>

```{r}
# create 'Duration' column, with values based on the existing 'Duration' column
df %>% 
  mutate(Duration = case_when(grepl("^[0-9]+h [0-9]+m$", Duration) ~ Duration, # if starts with one or more digits, followed by the letter "h", a space, one or more digits, and the letter "m", keep the original 'Duration' value
                              grepl("^[0-9]+h", Duration) ~ paste0(Duration, " 0m"), # if starts with one or more digits and the letter "h", append the string " 0m" to the end of the 'Duration' value
                              grepl("[0-9]+m$", Duration) ~ paste0("0h ", Duration), # if ends with one or more digits and the letter "m", prepend the string "0h " to the beginning of the 'Duration' value
                              # if none of the above, assign NA
                              TRUE ~ NA_character_)) -> df
```
<details>
  <summary>Python code</summary>
  ```python
  duration=list(df['Duration'])
  for i in range(len(duration)):
      if len(duration[i].split(' '))==2:
          pass
      else:
          if 'h' in duration[i]: # Check if duration contains only hour
               duration[i]=duration[i] + ' 0m' # Adds 0 minute
          else:
               duration[i]='0h '+ duration[i]
  ```
</details>

```{r}
head(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.head()
  ```
</details>

```{r}
hour <- function(x){
  ifelse(grepl("h", x), as.numeric(strsplit(x, 'h')[[1]][1]), 0)
}

minutes <- function(x){
  ifelse(is.na(x[2]), 0, as.numeric(sub(".*?(\\d+)m.*", "\\1", x[2])))
}
```
<details>
  <summary>Python code</summary>
  ```python
  def hour(x):
      return x.split(' ')[0][0:-1]
  
  def minutes(x):
      return x.split(' ')[1][0:-1]
  ```
</details>

```{r}
df$dur_hour <- sapply(df$Duration, hour)
```
<details>
  <summary>Python code</summary>
  ```python
  df['dur_hour']=df['Duration'].apply(hour)
  ```
</details>

```{r}
df$dur_min <- sapply(strsplit(df$Duration, ' '), minutes)
```
<details>
  <summary>Python code</summary>
  ```python
  df['dur_min']=df['Duration'].apply(minutes)
  ```
</details>

```{r}
head(df, 10)
```
<details>
  <summary>Python code</summary>
  ```python
  df.head(10)
  ```
</details>

```{r}
df <- df %>% 
  select(-Duration)
```
<details>
  <summary>Python code</summary>
  ```python
  drop_col(df,'Duration')
  ```
</details>

```{r}
str(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.dtypes
  ```
</details>

```{r}
df$dur_hour <- as.integer(df$dur_hour)
df$dur_min <- as.integer(df$dur_min)
```
<details>
  <summary>Python code</summary>
  ```python
  df['dur_hour'] = df['dur_hour'].astype(int)
  df['dur_min'] = df['dur_min'].astype(int)
  ```
</details>

```{r}
str(df)
```
<details>
  <summary>Python code</summary>
  ```python
  df.dtypes
  ```
</details>

#### Finding the categorical value

```{r}
column <- names(df)[sapply(df, is.character)]
column
```
<details>
  <summary>Python code</summary>
  ```python
  column=[column for column in df.columns if df[column].dtype=='object']
  column
  ```
</details>

#### Finding the continuous value

```{r}
continuous_col <- names(df)[!sapply(df, is.character)]
```
<details>
  <summary>Python code</summary>
  ```python
  continuous_col =[column for column in df.columns if df[column].dtype!='object']
  continuous_col
  ```
</details>

## Handling categorical data

```{r}
categorical <- df[column]
```
<details>
  <summary>Python code</summary>
  ```python
  categorical = df[column]
  ```
</details>

```{r}
head(categorical)
```
<details>
  <summary>Python code</summary>
  ```python
  categorical.head()
  ```
</details>

```{r}
table(categorical$Airline)
```
<details>
  <summary>Python code</summary>
  ```python
  categorical['Airline'].value_counts()
  ```
</details>

### Airline vs Price Analysis

```{r}
ggplot(data = df, aes(x = reorder(Airline, -Price), y = Price)) +
  geom_boxplot() +
  labs(x = "Airline", y = "Price")
```
<details>
  <summary>Python code</summary>
  ```python
  plt.figure(figsize=(15,8))
  sns.boxplot(x='Airline',y='Price',data=df.sort_values('Price',ascending=False))
  ```
</details>

```{r}
ggplot(data = df, aes(x = reorder(Total_Stops, -Price), y = Price)) +
  geom_boxplot() +
  labs(x = "Total_Stops", y = "Price")
```
<details>
  <summary>Python code</summary>
  ```python
  plt.figure(figsize=(15,8))
  sns.boxplot(x='Total_Stops',y='Price',data=df.sort_values('Price',ascending=False))
  ```
</details>

```{r}
Airline <- as.data.frame(model.matrix(~ Airline - 1, data = categorical))
new_names <- gsub("Airline", "", colnames(Airline))
Airline <- setNames(Airline, new_names)
```
<details>
  <summary>Python code</summary>
  ```python
  # As Airline is Nominal Categorical data we will perform OneHotEncoding
  Airline=pd.get_dummies(categorical['Airline'],drop_first=True)
  ```
</details>

```{r}
head(Airline)
```
<details>
  <summary>Python code</summary>
  ```python
  Airline.head()
  ```
</details>

```{r}
table(categorical$Source)
```
<details>
  <summary>Python code</summary>
  ```python
  categorical['Source'].value_counts()
  ```
</details>

```{r}
ggplot(data = df, aes(x = reorder(Source, -Price), y = Price)) +
  geom_boxplot() +
  labs(x = "Source", y = "Price")
```
<details>
  <summary>Python code</summary>
  ```python
  #Source vs Price
  plt.figure(figsize=(15,15))
  sns.catplot(x='Source',y='Price',data=df.sort_values('Price',ascending=False),kind='boxen')
  ```
</details>

```{r}
Source <- as.data.frame(model.matrix(~ Source - 1, data = categorical))
new_names <- gsub("Source", "", colnames(Source))
Source <- setNames(Source, new_names)

head(Source)
```
<details>
  <summary>Python code</summary>
  ```python
  #encoding of source column
  source=pd.get_dummies(categorical['Source'],drop_first=True)
  source.head()
  ```
</details>

```{r}
table(categorical$Destination)
```
<details>
  <summary>Python code</summary>
  ```python
  categorical['Destination'].value_counts()
  ```
</details>

```{r}
ggplot(data = df, aes(x = reorder(Destination, -Price), y = Price)) +
  geom_boxplot() +
  labs(x = "Destination", y = "Price")
```

<details>
  <summary>Python code</summary>
  ```python
  plt.figure(figsize=(15,8))
  sns.boxplot(x='Destination',y='Price',data=df.sort_values('Price',ascending=False))
  ```
</details>

```{r}
Destination <- as.data.frame(model.matrix(~ Destination - 1, data = categorical))
new_names <- gsub("Destination", "", colnames(Destination))
Destination <- setNames(Destination, new_names)

head(Destination)
```

<details>
  <summary>Python code</summary>
  ```python
  #encoding of destination column
  destination=pd.get_dummies(categorical['Destination'],drop_first=True)
  destination.head()
  ```
</details>

```{r}
table(categorical$Route)
```

<details>
  <summary>Python code</summary>
  ```python
  # now work on route column
  categorical['Route'].value_counts()
  ```
</details>

```{r}
categorical$Route1 <- sapply(strsplit(as.character(categorical$Route), "→"), "[", 1)
categorical$Route2 <- sapply(strsplit(as.character(categorical$Route), "→"), "[", 2)
categorical$Route3 <- sapply(strsplit(as.character(categorical$Route), "→"), "[", 3)
categorical$Route4 <- sapply(strsplit(as.character(categorical$Route), "→"), "[", 4)
categorical$Route5 <- sapply(strsplit(as.character(categorical$Route), "→"), "[", 5)
```

<details>
  <summary>Python code</summary>
  ```python
  categorical['Route1']=categorical['Route'].str.split('→').str[0]
  categorical['Route2']=categorical['Route'].str.split('→').str[1]
  categorical['Route3']=categorical['Route'].str.split('→').str[2]
  categorical['Route4']=categorical['Route'].str.split('→').str[3]
  categorical['Route5']=categorical['Route'].str.split('→').str[4]
  ```
</details>

Let's take a look at our dataframe as to how variables Route 1 through 5 look by each intermediate destination.

```{r}
categorical %>%
  head(5) %>%
  datatable(options = list(dom = 't', 
                           paging = FALSE,
                           scrollX = TRUE,
                           scrollCollapse = TRUE))
```

<details>
  <summary>Python code</summary>
  ```python
  categorical.head()
  ```
</details>

Removing the `Route` variable gives us guarantees that the same information is not passed twice. 

```{r}
categorical <- categorical %>% 
  select(-Route)
```

<details>
  <summary>Python code</summary>
  ```python
  drop_col(categorical,'Route')
  ```
</details>

Due to the fact that an airplane flight does not always have to go through more than one destination, it is necessary to check how many missing values we have for the Route variables.

```{r}
categorical %>% 
  summarise_all(~ sum(is.na(.))) %>% 
  pivot_longer(cols = everything(),
               names_to = 'Variable',
               values_to = 'No. of missing values')
```

<details>
  <summary>Python code</summary>
  ```python
  categorical.isnull().sum()
  ```
</details>

We have as many as 3491 missing values for the `Route3` variable, 9116 for the `Route4` variable and 10636 for the `Route5` variable. For each such missing value, we choose to replace it with `None`.

```{r}
categorical %>% colnames()
```

<details>
  <summary>Python code</summary>
  ```python
  categorical.columns
  ```
</details>

```{r}
categorical <- categorical %>% 
  mutate(Route3 = if_else(is.na(Route3), 'None', Route3),
         Route4 = if_else(is.na(Route4), 'None', Route4),
         Route5 = if_else(is.na(Route5), 'None', Route5))
```

<details>
  <summary>Python code</summary>
  ```python
  for i in ['Route3', 'Route4', 'Route5']:
    categorical[i].fillna('None',inplace=True)
  ```
</details>

It is also an important activity to check how many unique values each of the categorical variables has. In case one of the categorical variables contains as many classes as the number of variables then such a particular variable might not be useful for modeling. 

```{r}
categorical %>% 
  summarise_all(~ sum(is.na(.))) %>% 
  pivot_longer(cols = everything(),
               names_to = 'Variable',
               values_to = 'No. of missing values')
```

<details>
  <summary>Python code</summary>
  ```python
  categorical.isnull().sum()
  ```
</details>

```{r}
for (i in names(categorical)) {
  cat(paste0(i, " has total ", length(unique(categorical[[i]])), " categories\n"))
}
```

<details>
  <summary>Python code</summary>
  ```python
  for i in categorical.columns:
    print('{} has total {} categories'.format(i,len(categorical[i].value_counts())))
  ```
</details>

Let's now see how the observation counts break down by individual airplane arrival time and by flight price. As you can see, most of the observations are in the range of less than $20000 per flight, and we can see the highest number of arrivals for the late afternoon hours (6 and 7 pm).

```{r}
df %>% 
ggplot(aes(x = Arrival_Time_hour, y = Price)) + 
  geom_hex(bins = 15, aes(fill = after_stat(count))) +
  scale_fill_gradient(low = 'white', high = 'mediumaquamarine') +
  scale_y_continuous(breaks = seq(0, 80000, by = 10000)) +
  theme(panel.background = element_rect(fill = "white"))
```

<details>
  <summary>Python code</summary>
  ```python
  df.plot.hexbin(x='Arrival_Time_hour',y='Price',gridsize=15)
  ```
</details>

Machine learning algorithms can only cope on numerical values. Therefore, consistently, categorical variables must be decoded from word values to numeric values. 

```{r}
for (i in c("Route1", "Route2", "Route3", "Route4", "Route5")){
  categorical[[i]] <- factor(categorical[[i]]) %>% as.numeric()
}
```

<details>
  <summary>Python code</summary>
  ```python
  # Applying label encoder
  from sklearn.preprocessing import LabelEncoder
  encoder = LabelEncoder()
  for i in ['Route1', 'Route2', 'Route3', 'Route4', 'Route5']:
    categorical[i]=encoder.fit_transform(categorical[i])
  ```
</details>

```{r}
categorical %>% 
  head(5)
```

<details>
  <summary>Python code</summary>
  ```python
  categorical.head()
  ```
</details>

```{r}
categorical <- categorical %>% 
  select(-Additional_Info)
```

<details>
  <summary>Python code</summary>
  ```python
  drop_col(categorical,'Additional_Info')
  ```
</details>


```{r}
categorical %>% 
  select(Total_Stops) %>% 
  unique()
```

<details>
  <summary>Python code</summary>
  ```python
  categorical['Total_Stops'].unique()
  ```
</details>


```{r}
map_levels <- c('non-stop', '1 stop', '2 stops',  '3 stops', '4 stops')
map_values <- c(0, 1, 2, 3, 4)
categorical[['Total_Stops']] <- factor(categorical$Total_Stops, levels = map_levels, labels = map_values) %>% as.numeric() - 1
```

<details>
  <summary>Python code</summary>
  ```python
  # encoding Total stops
  dict={'non-stop':0, '2 stops':2, '1 stop':1, '3 stops':3, '4 stops':4}
  categorical['Total_Stops']=categorical['Total_Stops'].map(dict)
  ```
</details>

```{r}
categorical %>% 
  select(Total_Stops)
```

<details>
  <summary>Python code</summary>
  ```python
  categorical['Total_Stops']
  ```
</details>

```{r}
categorical <- categorical %>% 
  select(-Source, -Destination, -Airline)
```

<details>
  <summary>Python code</summary>
  ```python
  drop_col(categorical,'Source')
  drop_col(categorical,'Destination')
  drop_col(categorical,'Airline')
  ```
</details>

<span style="font-size:x-large;display:table;margin: 0 auto;">After all preprocessing, Our data is ready for the modeling</span>

At this point it is necessary to collect all minor dataframes on which separately performed ordering operations, decoding of categorical variables, calculation of numerical values from dates or transformation by appropriate mathematical functions.

```{r}
final_df <- cbind(categorical, Airline, Source, Destination, df[continuous_col])
final_df <- final_df[!duplicated(names(final_df))]
```

<details>
  <summary>Python code</summary>
  ```python
  final_df=pd.concat([categorical,Airline,source,destination,df[continuous_col]],axis=1)
  ```
</details>

```{r}
final_df %>% 
  head(5) %>%
    datatable(options = list(dom = 't', 
                           paging = FALSE,
                           scrollX = TRUE,
                           scrollCollapse = TRUE))
```

<details>
  <summary>Python code</summary>
  ```python
  final_df.head()
  ```
</details>

```{r}
options(max.print = 33)
final_df %>% 
  head(5) %>% 
    datatable(options = list(dom = 't', 
                           paging = FALSE,
                           scrollX = TRUE,
                           scrollCollapse = TRUE))
```

<details>
  <summary>Python code</summary>
  ```python
  pd.set_option('display.max_columns',33)
  final_df.head()
  ```
</details>

## Check For Outliers

Analysis of outlier observations provides a deeper understanding of the distribution of all observations. It is also used to detect some incomprehensible behavior among the data, some anomalies that machine learning models may not expect. That's why it's worth looking at the distribution of the dependent variable on the value of flight. 

```{r}
plot <- function(data, col) {
  
  p1 <- ggplot(data, aes(x = .data[[col]])) +
    geom_histogram(aes(y = ..density..), fill = "lightblue", alpha = 0.5) +
    geom_density(color = "darkblue", linewidth = 1) +
    labs(x = col, y = "Frequency") +
    theme_bw()
  
  p2 <- ggplot(data, aes(y = .data[[col]])) +
    geom_boxplot(fill = "darkblue", color = "black", alpha = 0.5) +
    labs(x = "", y = col) +
    theme_bw() +
    coord_flip()
  
  ggarrange(p1, p2, ncol = 1, nrow = 2)
}
```

<details>
  <summary>Python code</summary>
  ```python
  def plot(data,col):
    fig,(ax1,ax2)=plt.subplots(2,1)
    sns.distplot(data[col],ax=ax1)
    sns.boxplot(data[col],ax=ax2)
  ```
</details>

```{r message=FALSE, warning=FALSE}
plot(final_df,'Price')
```

<details>
  <summary>Python code</summary>
  ```python
  plot(final_df,'Price')
  ```
</details>

## Handling outliers

As you can see from the graph above, there are outliers in our dataset that can disrupt the inference of machine learning models. We therefore decide to replace all values above the price of $40000 with the median value for the entire set. After this change, let's look at how the distribution of the dependent variable now looks. 

```{r}
final_df$Price <- ifelse(final_df$Price >= 40000, median(final_df$Price), final_df$Price)
```

<details>
  <summary>Python code</summary>
  ```python
  final_df['Price']=np.where(final_df['Price']>=40000,final_df['Price'].median(),final_df['Price'])
  ```
</details>

```{r message=FALSE, warning=FALSE}
plot(final_df,'Price')
```

<details>
  <summary>Python code</summary>
  ```python
  plot(final_df,'Price')
  ```
</details>

## Feature Selection

Feature selection involves identifying the optimal features and establishing a strong relationship with the independent variable.

Firstly, the dataset is divided into X and y columns for further modeling purposes.

```{r message=FALSE, warning=FALSE}
# Extract all columns but Price as X
X <- final_df[, !(names(final_df) == "Price")]

# Extract Price column as y
y <- df$Price
```

<details>
  <summary>Python code</summary>
  ```python
  X=final_df.drop('Price',axis=1)
  y=df['Price']
  ```
</details>

Suggested approach for feature importance in the base study was Mutual Information of variables, which is a measure of the similarity between two labels of the same data. There are several options in R to calculate Mutual Information - for the purpose of this project calculation or MI scores with *praznik* package is proposed.


```{r, message=FALSE, warning=FALSE}
# Load library
library(praznik)

# Calculate Mutual Information of variables
miScores_result <- miScores(X, y, threads = 0)
miScores_result
```


<details>
  <summary>Python code</summary>
  ```python
  from sklearn.feature_selection import mutual_info_classif
  mutual_info_classif(X,y)
  ```
</details>


MI scores are sorted in decreasing order in a datatable as follows:

```{r, message=FALSE, warning=FALSE}
# MI scores into a dataframe
miScores_df <- data.frame()
miScores_df <- data.frame(feature = names(miScores_result), miScore = unlist(miScores_result))
miScores_df <- miScores_df[order(miScores_df$miScore, decreasing = TRUE), ]

# MI scores from a dataframe to datatable
options(max.print = 33)
miScores_df %>% 
  head(33) %>% 
    datatable(options = list(dom = 't', 
                           paging = FALSE,
                           scrollX = TRUE,
                           scrollCollapse = TRUE),
              rownames = FALSE)
```



<details>
  <summary>Python code</summary>
  ```python
  imp = pd.DataFrame(mutual_info_classif(X,y),index=X.columns)
  imp
  imp.columns=['importance']
  imp.sort_values(by='importance',ascending=False)
  ```
</details>


The results of MI scores slightly differ from the values obtained while using Python. It shows that duration in hours `dur_hour`, total number of stops `Total_Stops`, `Route3` and `Route2` variables are the most significant. It was expected that top features would be `Route3`, `Route2`, `Total_Stops` and `Route1`. Obtained MI scores also showed that `Vistara Premium Economy`, `Trujet`, `Multiple carriers` are the least significant, however scoring above 0. It was expected that `Jet Airways Business` and `Trujet` would get MI score equal to 0, followed by `Vistara Premium Economy`, `Multiple carriers`, `GoAir` with the smallest scores. However, the differences do not change the overall assessment of features importance for further modelling. All features remain.


## Models

Various Machine Learning models were trained and tested on the preprocessed dataset in order to predict the flight prices between given destinations. For the purpose of reproducing the base project, Logistic Regression, K-nearest Neighbors Regression, Decision Tree, Gradient Boosting models were used to solve the regression problem.

Firstly, the dataset was split into train and test samples in proportion 80% - 20% according to the standard procedure. 

```{r message=FALSE, warning=FALSE}
# Set random seed for reproducibility
set.seed(123)

# Load library
library(caret)

# Split the dataset into train and test samples
train_indices <- createDataPartition(y, times = 1, p = 0.8, list = FALSE)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- y[train_indices]
y_test <- y[-train_indices]
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.model_selection import train_test_split
  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=123)
  ```
</details>


Further, two function for modeling were created based on the *caret* package. `train_predict` function fits the model to the training data within the selected method (LR, KNN, DT, GDB), make predictions on the test data and returns them.

```{r message=FALSE, warning=FALSE}
train_predict <- function(method, X_train, y_train, X_test) {
  # Fit the model on the training data
  model <- train(x = X_train, y = y_train, method = method)
  
  # Make predictions on the test data
  predictions <- predict(model, newdata = X_test)
  
  # Return predictions
  return(predictions)
}
```


`calculate_metrics` aggregates measurement metrics of a model based on the predictions. These are: R-squared, Mean Absolute Error (MAE), Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).


```{r message=FALSE, warning=FALSE}
# Load library
library(Metrics)

calculate_metrics <- function(predictions, y_test) {
  
  r2score <- R2(predictions, y_test)
  print(paste("R-squared score:", r2score))
  
  mae <- MAE(predictions, y_test)
  print(paste("Mean Absolute Error:", mae))
  
  mse <- mean((y_test - predictions)^2)
  print(paste("Mean Squared Error:", mse))
  
  rmse <- RMSE(predictions, y_test)
  print(paste("Root Mean Squared Error:", rmse))
}
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
  def predict(ml_model):
    print('Model is: {}'.format(ml_model))
    model= ml_model.fit(X_train,y_train)
    print("Training score: {}".format(model.score(X_train,y_train)))
    predictions = model.predict(X_test)
    print("Predictions are: {}".format(predictions))
    print('\n')
    r2score=r2_score(y_test,predictions) 
    print("r2 score is: {}".format(r2score))
          
    print('MAE:{}'.format(mean_absolute_error(y_test,predictions)))
    print('MSE:{}'.format(mean_squared_error(y_test,predictions)))
    print('RMSE:{}'.format(np.sqrt(mean_squared_error(y_test,predictions))))
     
    sns.distplot(y_test-predictions) 
  ```
</details>



### Logistic Regression

```{r message=FALSE, warning=FALSE}
# Set random seed for reproducibility
set.seed(123)

# Choose glm method for Logistic Regression
method <- "glm"

# Train and test model to obtain predictions
predictions_lm <- train_predict(method, X_train, y_train, X_test)
```

```{r message=FALSE, warning=FALSE}
# Calculate measurement metrics of the LR model
calculate_metrics(predictions_lm, y_test)
```

```{r message=FALSE, warning=FALSE}
# Plot the distribution of residuals
residuals_lm <- y_test - predictions_lm

# Create a dataframe with the residuals
data <- data.frame(residuals = residuals_lm)

# Plot the histogram of residuals
ggplot(data, aes(x = residuals, y = ..density..)) +
  geom_histogram(fill = "lightblue", alpha = 0.7, color = "lightblue", bins = 100) +
  geom_density(color = "darkblue", size = 0.5) +
  labs(title = "Residuals Distribution Logistic Regression", x = "Price", y = "Density") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "white"))
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.linear_model import LogisticRegression
  predict(LogisticRegression()) 
  sns.distplot(y_test-predictions) 
  ```
</details>

The results obtained by the Logistic Regression model in this study varies from the results from the base project, therefore it was not possible to obtain full reproducibility. More specifically, R2 explains around 63% of the variance in the flight price, which is more than in the base study (44%). It suggests a reasonable level of variance explained by the model, while the MAE, MSE, and RMSE values are lower in this study than in the base, which proves that less prediction errors and better overall model performance than expected.


### K-nearest Neighbors Regression

```{r message=FALSE, warning=FALSE}
# Set random seed for reproducibility
set.seed(123)

# Choose knn method for K-nearest Neighbors Regression
method <- "knn"

# Train and test model to obtain predictions
predictions_knn <- train_predict(method, X_train, y_train, X_test)
```

```{r message=FALSE, warning=FALSE}
# Calculate measurement metrics of the KNN model
calculate_metrics(predictions_knn, y_test)
```

```{r message=FALSE, warning=FALSE}
# Plot the distribution of residuals
residuals_knn <- y_test - predictions_knn

# Create a data frame with the residuals
data <- data.frame(residuals = residuals_knn)

# Plot the histogram of residuals
ggplot(data, aes(x = residuals, y = ..density..)) +
  geom_histogram(fill = "lightblue", alpha = 0.7, color = "lightblue", bins = 100) +
  geom_density(color = "darkblue", size = 0.5) +
  labs(title = "Residuals Distribution KNN", x = "Price", y = "Density") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "white"))
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.linear_model import KNeighborsRegressor
  predict(KNeighborsRegressor())
  sns.distplot(y_test-predictions)
  ```
</details>


The results obtained by the K-nearest Neighbors Regression model in this study varies from the results from the base project, therefore it was not possible to obtain full reproducibility. However, this time the reproduced model underperformed compared to the base model. The R2 value explains around 62% of the variance in the flight price, which is slightly lower than expected (65%). This indicates that the model performs worse in capturing and explaining the variation in the target variable. Therefore, MAE, MSE, and RMSE values are higher in this study compared to the base, indicating no reduction in prediction errors. It is worth mentioning that the differences between reprocuded and the original model are minor.


### Decision Tree

```{r message=FALSE, warning=FALSE}
# Set random seed for reproducibility
set.seed(123)

# Choose rpart method for Decision Tree
method <- "rpart"

# Train and test model to obtain predictions
predictions_dt <- train_predict(method, X_train, y_train, X_test)
```

```{r message=FALSE, warning=FALSE}
# Calculate measurement metrics of the DT model
calculate_metrics(predictions_dt, y_test)
```

```{r message=FALSE, warning=FALSE}
# Plot the distribution of residuals
residuals_dt <- y_test - predictions_dt

# Create a data frame with the residuals
data <- data.frame(residuals = residuals_dt)

# Plot the histogram of residuals
ggplot(data, aes(x = residuals, y = ..density..)) +
  geom_histogram(fill = "lightblue", alpha = 0.7, color = "lightblue", bins = 100) +
  geom_density(color = "darkblue", size = 0.5) +
  labs(title = "Residuals Distribution Decision Tree", x = "Price", y = "Density") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "white"))
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.linear_model import DecisionTreeRegressor
  predict(predict(DecisionTreeRegressor())
  sns.distplot(y_test-predictions) 
  ```
</details>


The results obtained by the Decision Tree model in this study significantly deviate from the base project, leading to challenges in achieving full reproducibility. The reproduced model demonstrates inferior performance compared to the base model, as indicated by a low R2 value of around 46%, failing to effectively capture and explain the variation in the flight price (expected 74%). Moreover, the reproduced model exhibits higher MAE, MSE, and RMSE values, suggesting an inability to reduce prediction errors. It is evident that the reproduced Decision Tree model falls short in terms of accuracy and performance. These disparities may stem from variations in data preprocessing, feature selection, or model hyperparameters.



### Gradient Boosting

```{r message=FALSE, warning=FALSE, include=FALSE}
# Set random seed for reproducibility
set.seed(123)

# Choose gbm method for Gradient Boosting
method <- "gbm"

# Train and test model to obtain predictions
predictions_gbm <- train_predict(method, X_train, y_train, X_test)
```

```{r message=FALSE, warning=FALSE}
# Calculate measurement metrics of the GDB model
calculate_metrics(predictions_gbm, y_test)
```

```{r message=FALSE, warning=FALSE}
# Plot the distribution of residuals
residuals_gbm <- y_test - predictions_gbm

# Create a data frame with the residuals
data <- data.frame(residuals = residuals_gbm)

# Plot the histogram of residuals
ggplot(data, aes(x = residuals, y = ..density..)) +
  geom_histogram(fill = "lightblue", alpha = 0.7, color = "lightblue", bins = 100) +
  geom_density(color = "darkblue", size = 0.5) +
  labs(title = "Residuals Distribution Gradient Boosting", x = "Price", y = "Density") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "white"))
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.linear_model import GradientBoostingRegressor
  predict(GradientBoostingRegressor())
  sns.distplot(y_test-predictions) 
  ```
</details>


The reproduced Gradient Boosting model deviates slightly from the base project, resulting in inferior performance. The R2 value, which explains the variance in flight prices, is equal to 75%, a bit lower than the expected 81%. Additionally, higher values of MAE, MSE, and RMSE indicate increased prediction errors. These differences are minor. Overall, the performance of Gradient Boosting model was the best compared to other models, which was expected from the base paper. The main insight and conclusion from the study was reproduced successfully.


## Hypertunning the model

In the base study, the hypertunning was obtained using `RandomizedSearchCV` - a utility class in `scikit-learn` in Python. It performs random search hyperparameter tuning for machine learning models. It helps in finding the best combination of hyperparameters for a given model by randomly sampling from a defined parameter grid. In R, the `trainControl` object is created first to conduct cross-validation.

Random search was implemented on Gradient Boosting Regressor model developed in previous subsection, as that was the best performing model out of the reproduced models. K-fold cross validation was set to `cv = 3`, which means that the model will be trained and evaluated using cross-validation with 3 folds, enabling a robust assessment of the model's performance and preventing overfitting.

```{r message=FALSE, warning=FALSE}
# Create a trainControl object for cross-validation
ctrl <- trainControl(method = "cv", number = 3)
```

<details>
  <summary>Python code</summary>
  ```python
  from sklearn.model_selection import RandomizedSearchCV
  ```
</details>

Then, parameter grid is created. This grid specifies the values or ranges of values to explore for each hyperparameter of the Gradient Boosting Regressor.

```{r message=FALSE, warning=FALSE}
# Define the parameter grid for hypertuning
param_grid <- expand.grid(
  n.trees = c(100, 200, 300), #number of boosting iterations
  interaction.depth = c(3, 5, 7), #maximum depth of an individual tree in the gradient boosting ensemble
  shrinkage = c(0.1, 0.01, 0.001), #controls the learning rate of each tree in the ensemble 
  n.minobsinnode = c(10, 20, 30) #sets the minimum number of samples required in a terminal node of a tree
)
```

<details>
  <summary>Python code</summary>
  ```python
  random_grid = {
    'n_estimators' : [100, 120, 150, 180, 200,220],
    'max_features':['auto','sqrt'],
    'max_depth':[5,10,15,20],
    }
  ```
</details>

Further, the new model is trained using random search with `tuneGrid` defined before and `trControl` as train control object defined initially. The outcome is the set of the best parameters.

```{r message=FALSE, warning=FALSE}
# Create a Random Search using the train function from the caret package
gbm_random <- train(
  x = X_train,
  y = y_train,
  method = "gbm",
  tuneGrid = param_grid,
  trControl = ctrl
)

# Print the best parameters found by the random search
print(gbm_random$bestTune)
```


Based on the best parameters, new predictions are made and new measurement metrics are calculated.

```{r message=FALSE, warning=FALSE}
# Use the best parameters to make predictions on the test set
predictions <- predict(gbm_random, newdata = X_test)
```

```{r message=FALSE, warning=FALSE}
calculate_metrics(predictions, y_test)
```

```{r message=FALSE, warning=FALSE}
# Plot the distribution of residuals
residuals<- y_test - predictions

# Create a data frame with the residuals
data <- data.frame(residuals = residuals)

# Plot the histogram of residuals
ggplot(data, aes(x = residuals, y = ..density..)) +
  geom_histogram(fill = "lightblue", alpha = 0.7, color = "lightblue", bins = 100) +
  geom_density(color = "darkblue", size = 0.5) +
  labs(title = "Residuals Distribution Gradient Boosting after Hypertunning", x = "Price", y = "Density") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "white"))
```

The results of the tuned model are visible from both, the metrics, as well as the plot of residuals distribution. After customizing the model with the set of the best parameters, the R2 value in creased from 75% to 82%, which implies better explainability of the adjusted model of analyzed data. Therefore, the predictions of flight prices are more accurate. The errors, i.e. MAE, MSE, RMSE decreased, which influenced better fir of the residuals on the plot compared to the basic GDB model. Summing up, the random search hypertunning was successfull as expected from the base study. The approach was correctly reproduced into another machine learning model.


# Extended modeling: XGBoost

As an extension to the selected base models, this study propose another Machine Learning algorithm for flight price predictions, XGBoost.

XGBoost enhances the traditional gradient boosting algorithm by incorporating several innovative features and optimizations, resulting in improved accuracy and speed, such as: L1 and L2 regularization, tree pruning or customizable objective functions.


```{r message=FALSE, warning=FALSE, include=FALSE}
# Set random seed for reproducibility
set.seed(123)

# Choose xgbTree method for XGBoost
method <- "xgbTree"

# Set up tuning grid with default values
grid <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Create a trainControl object for cross-validation
ctrl <- trainControl(method = "none", number = 5, verboseIter = FALSE, allowParallel = FALSE)

# Train and test model to obtain predictions
model_xbg <- train(
  x = X_train,
  y = y_train,
  trControl = ctrl,
  method = "xgbTree",
  tuneGrid = grid
)
```

```{r}
# Train and test model to obtain predictions
predictions_xgb <- predict(model_xbg, newdata = X_test)
```

```{r message=FALSE, warning=FALSE}
# Calculate measurement metrics of the XGBoost model
calculate_metrics(predictions_xgb, y_test)
```

Out of all performed machine learning models, XGBoost obtained the best results, the highest R2 of value 86% and lower prediction errors.

In conclusion, the best performing machine learning algorithm was XGBoost, followed by tuned Gradient Boosting Regressor.

